package de.uni_mannheim.desq.examples.spark

import java.io.FileOutputStream
import java.util.concurrent.TimeUnit

import com.google.common.base.Stopwatch
import de.uni_mannheim.desq.Desq._
import de.uni_mannheim.desq.converters.nyt.{ConvertNyt, NytUtil}
import de.uni_mannheim.desq.dictionary.Dictionary
import de.uni_mannheim.desq.mining.spark.DesqDataset
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.JavaConversions._

/**
  * Created by rgemulla on 03.10.2016.
  */
object DesqBuilderExample {

  def nyt()(implicit sc: SparkContext) {
    // create the dataset
    val lines = sc.textFile("data-local/nyt-1991-data.del")
    val data = DesqDataset.buildFromStrings(lines.map(s => s.split(" ")))

    // save it
    val savedData = data.save("data-local/nyt-1991-data")
    savedData.print(5)
    println("--")

    // load it
    val loadedData = DesqDataset.load("data-local/nyt-1991-data")
    loadedData.print(5)
    println("--")

    // test correctness of counts
    val fullDict = Dictionary.loadFrom("data-local/nyt-1991-dict.avro.gz")
    for (fid <- fullDict.fids) {
      if (fullDict.childrenOf(fid).size() == 0) {
        // only verify leafs
        val otherFid = loadedData.dict.fidOf(fullDict.sidOfFid(fid))
        if (otherFid != -1 && // the created dictionary may not contain some items (those which do not occur in the data)
          (fullDict.cfreqOf(fid) != loadedData.dict.cfreqOf(otherFid) ||
            fullDict.dfreqOf(fid) != loadedData.dict.dfreqOf(otherFid))) {
          println("Incorrect item: " + fullDict.sidOfFid(fid) + " " + loadedData.dict.fidOf(otherFid))
        }
      }
    }
    println("DONE")
  }

  def nyt_scala()(implicit sc: SparkContext) {

    val nyt_avro = "data-local/NYTimesProcessed/results/2007/01/"
    val nytJavaOutDir = "data-local/processed/sparkconvert/2007_java/"
    val nytConverter = new ConvertNyt

    print("Converting the raw data using the Java NYT Converter... ")
    val javaTime = Stopwatch.createStarted()
    nytConverter.buildDesqDataset(nyt_avro, nytJavaOutDir, false)
    javaTime.stop()
    println(javaTime.elapsed(TimeUnit.MILLISECONDS) + "ms")

    print("Converting the raw data using the Scala NYT Converter... ")
    val scalaTime = Stopwatch.createStarted()
    val articles_raw = NytUtil.loadArticlesFromFile(nyt_avro).map(r => (42L,r)).flatMapValues(f=>f.getSentences)
    val data = DesqDataset.buildFromSentencesWithID(articles_raw)
    scalaTime.stop()
    println(scalaTime.elapsed(TimeUnit.MILLISECONDS) + "ms")

    val dataSaved = data.save("data-local/processed/sparkconvert/2007_scala")
    dataSaved.print(5)
    println("--")

    val loadedData = DesqDataset.load("data-local/processed/sparkconvert/2007_scala")
    loadedData.print(5)
    println("--")
    println(loadedData.sequences.filter(f=>f.id < 12).count())

    val fullDict = Dictionary.loadFrom("data-local/processed/sparkconvert/2007_java/all/dict.avro.gz")
    for (fid <- fullDict.fids) {
      if (fullDict.childrenOf(fid).size() == 0) {
        // only verify leafs
        val otherFid = loadedData.dict.fidOf(fullDict.sidOfFid(fid))
        if (otherFid != -1 && // the created dictionary may not contain some items (those which do not occur in the data)
          (fullDict.cfreqOf(fid) != loadedData.dict.cfreqOf(otherFid) ||
            fullDict.dfreqOf(fid) != loadedData.dict.dfreqOf(otherFid))) {
          println("Incorrect item: " + fullDict.sidOfFid(fid) + " " + fid + " " + otherFid)
        }
      }
    }
    println("DONE")
  }

  def findBug()(implicit sc: SparkContext): Unit = {

    print("Loading DesqDataset from Disk\n")
    val loadedData = DesqDataset.load("data-local/processed/sparkconvert/2007_scala")
    print("Writing Dict of DesqDataset to Disk\n")
    val scalaDict = loadedData.dict
    print("Writing dict.json\n")
    scalaDict.write("data-local/processed/sparkconvert/2007_sc/dict.json");
    print("Writing dict.avro.gz\n")
    scalaDict.write("data-local/processed/sparkconvert/2007_sc/dict.avro.gz");

    print("Loading DesqDataset dict.avro.gz from Disk\n")
    val errorDict = Dictionary.loadFrom("data-local/processed/sparkconvert/2007_sc/dict.json")

    print("Loading Dictionary generated by Java Class\n")
    val fullDict = Dictionary.loadFrom("data-local/processed/sparkconvert/2007_java/all/dict.avro.gz")

    val data = new DesqDataset(loadedData.sequences, fullDict)
    data.dict.recomputeFids()
    val data2 = DesqDataset.loadFromDelFile("data-local/processed/sparkconvert/2007_java/all/gid.del", fullDict)
    data2.dict.recomputeFids()
//
//    print("Compare DesqDatasetDictionary with Java generated Dictionary\n")
//    compare(fullDict, loadedData.dict)
//    print("____________________________________________________________\n")
//
//    print("Compare DesqDatasetDictionary (recomputetFids) with Java generated dict\n")
//    loadedData.dict.recomputeFids()
//    compare(fullDict, loadedData.dict)
//    print("____________________________________________________________\n")
//
//    print("Compare DesqDatasetDictionary (recomputetFids) with Java generated dict (recomputetFids)\n")
//    fullDict.recomputeFids()
//    compare(fullDict, loadedData.dict)
//    print("____________________________________________________________\n")

    print("Compare Java Loaded DesqDataset.Dict with Java generated dict\n")
    compare(data2.dict, errorDict)
    print("____________________________________________________________\n")

//    print("Compare Java Loaded DesqDataset.Dict with Java generated dict\n")
//    errorDict.recomputeFids()
//    compare(errorDict, fullDict)
//    print("____________________________________________________________\n")



    def compare(fullDict: Dictionary, errorDict:Dictionary){
    for (fid <- fullDict.fids) {
      if (fullDict.childrenOf(fid).size() == 0) {
        // only verify leafs
        val otherFid = errorDict.fidOf(fullDict.sidOfFid(fid))
        if (otherFid != -1 && // the created dictionary may not contain some items (those which do not occur in the data)
          (fullDict.cfreqOf(fid) != errorDict.cfreqOf(otherFid) ||
            fullDict.dfreqOf(fid) != errorDict.dfreqOf(otherFid))) {
          println("Incorrect item: " + fullDict.sidOfFid(fid) + " " + fid + " " + otherFid + s" difference in cfreq ${fullDict.cfreqOf(fid)} ${errorDict.cfreqOf(otherFid)} || dfreq ${fullDict.dfreqOf(fid)} ${errorDict.dfreqOf(otherFid)}")
        }
      }
    }
    println("DONE")
  }
  }

  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName(getClass.getName).setMaster("local")
    initDesq(conf)
    implicit val sc = new SparkContext(conf)
    nyt_scala()
//    findBug()
  }
}
